{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a combination of:\n",
    "* https://bj.lianjia.com/\n",
    "* https://bj.ke.com/\n",
    "* https://bj.fang.com/\n",
    "\n",
    "\n",
    "Automation of collecting house for sale data and some analysis if needed\n",
    "\n",
    "Since in Beijing new construction is pretty far we are not cosidering it now.    \n",
    "Mainly focus on used house/apt around Shuangjing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import matplotlib\n",
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 链家找房"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urllist contains all urls that should be go over\n",
    "# It includes all different areas: Shuangjing, Jinsong, etc\n",
    "\n",
    "urllist= [\"https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/\",# Shuang jing\n",
    "          \"https://bj.lianjia.com/ershoufang/rs%E5%8A%B2%E6%9D%BE/\" # Jin song\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LJ_urlmaker(urllist, nbr = 0,\n",
    "             pgnbr = 1,\n",
    "             price = 0,# 售价\n",
    "             bedroom = 0,# 房型\n",
    "             area = 0,# 面积\n",
    "             age = 0,# 楼龄\n",
    "             lift = 0# 电梯\n",
    "            ):\n",
    "    \n",
    "    url = urllist[nbr]\n",
    "    url1 = url.split(\"/\")[-2]\n",
    "    \n",
    "    \n",
    "    if price == 1:\n",
    "        price_suffix = \"p1\" # <200\n",
    "    elif price  == 2:\n",
    "        price_suffix = \"p2\" # 200-250\n",
    "    elif price  == 3:\n",
    "        price_suffix = \"p3\" # 250-300\n",
    "    elif price  == 4:\n",
    "        price_suffix = \"p4\" # 300-400\n",
    "    elif price  == 5:\n",
    "        price_suffix= \"p5\" # 400-500\n",
    "    elif price  == 6:\n",
    "        price_suffix= \"p6\" # 500-800\n",
    "    elif price  == 7:\n",
    "        price_suffix= \"p7\" # 800-1000\n",
    "    elif price  == 8:\n",
    "        price_suffix= \"p8\" # >1000\n",
    "    else:\n",
    "        price_suffix =\"\"\n",
    "        \n",
    "    if bedroom == 1:\n",
    "        bd_suffix =  \"l1\"\n",
    "    elif bedroom  == 2:\n",
    "        bd_suffix =  \"l2\"\n",
    "    elif bedroom  == 3:\n",
    "        bd_suffix =  \"l3\"\n",
    "    elif bedroom  == 4:\n",
    "        bd_suffix =  \"l4\"\n",
    "    elif bedroom  == 5:\n",
    "        bd_suffix =  \"l5\"\n",
    "    else:\n",
    "        bd_suffix =\"\"\n",
    "\n",
    "    if area == 1:\n",
    "        area_suffix = \"a1\" # <50\n",
    "    elif area  == 2:\n",
    "        area_suffix = \"a2\" # 50-70\n",
    "    elif area  == 3:\n",
    "        area_suffix = \"a3\" # 70-90\n",
    "    elif area  == 4:\n",
    "        area_suffix = \"a4\" # 90-110\n",
    "    elif area  == 5:\n",
    "        area_suffix= \"a5\" # 110-130\n",
    "    elif area  == 6:\n",
    "        area_suffix= \"a6\" # 130-150\n",
    "    elif area  == 7:\n",
    "        area_suffix= \"a7\" # 150-200\n",
    "    elif area  == 8:\n",
    "        area_suffix= \"a8\" # >200\n",
    "    else:\n",
    "        area_suffix =\"\"\n",
    "\n",
    "    if age == 1:\n",
    "        age_suffix =  \"y1\" # <5\n",
    "    elif bedroom  == 2:\n",
    "        age_suffix =  \"y2\" # <10\n",
    "    elif bedroom  == 3:\n",
    "        age_suffix =  \"y3\" # <15\n",
    "    elif bedroom  == 4:\n",
    "        age_suffix =  \"y4\" # <20\n",
    "    elif bedroom  == 5:\n",
    "        age_suffix =  \"y5\" # >20\n",
    "    else:\n",
    "        age_suffix =\"\"\n",
    "\n",
    "    if lift == 1:\n",
    "        lift_suffix =  \"ie2\" # has lift\n",
    "    elif lift  == 2:\n",
    "        lift_suffix =  \"ie1\" # no lift\n",
    "    else:\n",
    "        lift_suffix =\"\"\n",
    "    \n",
    "    \n",
    "    url2 = \"https://bj.lianjia.com/ershoufang/\" + \"pg\" + str(pgnbr)+ price_suffix + bd_suffix + area_suffix + age_suffix + lift_suffix + url1 + \"/\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "# a = Lianjia_urlmaker(urllist, nbr = 0,\n",
    "#              pgnbr = 1,\n",
    "#              price = 0,# 售价\n",
    "#              bedroom = 0,# 房型\n",
    "#              area = 0,# 面积\n",
    "#              age = 0,# 楼龄\n",
    "#              lift = 0# 电梯\n",
    "#             )\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_contents(url_to_scrape):\n",
    "    headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'\n",
    "            }\n",
    "    \n",
    "    response = requests.get(url_to_scrape,headers=headers,timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        return response\n",
    "    else:\n",
    "        print(\"internet problem!\")\n",
    "\n",
    "# aa = get_url_contents(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LJ_get_dataframe(response):    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    comment_list = soup.find_all(\"div\",{\"class\":\"title\"})\n",
    "    location_list = soup.find_all(\"div\",{\"class\":\"positionInfo\"})\n",
    "    address_list = soup.find_all(\"div\",{\"class\":\"address\"})\n",
    "#     houseInfo_list = soup.find_all(\"div\",{\"class\":\"houseInfo\"})\n",
    "    followInfo_list = soup.find_all(\"div\",{\"class\":\"followInfo\"})\n",
    "    tag_list = soup.find_all(\"div\",{\"class\":\"tag\"})\n",
    "    priceInfo_list = soup.find_all(\"div\",{\"class\":\"priceInfo\"})\n",
    "    \n",
    "    for i in range(len(location_list)):\n",
    "            \n",
    "#         cut address and price information\n",
    "        address_info = address_list[i].text.split(\"|\")\n",
    "        price_info = priceInfo_list[i].text.split(\"单价\")\n",
    "#         get url of this apt\n",
    "        url = comment_list[i].a['href']\n",
    "        \n",
    "        d = {\n",
    "             \"name\" : comment_list[i].text, \n",
    "             \"area\" : address_info[1].split(\"平\")[0], \n",
    "             \"direction\": address_info[2],\n",
    "             \"location\":location_list[i].text,\n",
    "             \"floorplan\": address_info[0], \n",
    "             \"high/low\": address_info[4], \n",
    "             \"year\": address_info[5].split(\"年\")[0],\n",
    "             \"decoration\": address_info[3],\n",
    "             \"type\": address_info[6],\n",
    "            \"follow_info\":followInfo_list[i].text,\n",
    "            \"tag\": tag_list[i].text,\n",
    "             \"url\": url,\n",
    "             \"total_price\": price_info[0].split(\"万\")[0],\n",
    "             \"unit_price\": price_info[1].split(\"元\")[0],\n",
    "            \"source\":\"链家\"\n",
    "            }\n",
    "        df = df.append(d, ignore_index=True)\n",
    "#         print(\"get dataframe finished!\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# LJ_get_dataframe(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(urllist): \n",
    "    \n",
    "    result_df1 = pd.DataFrame()\n",
    "    for j in range(len(urllist)):\n",
    "        url_1 =LJ_urlmaker(urllist, nbr = 0,\n",
    "             pgnbr = 1,\n",
    "             price = 0,# 售价\n",
    "             bedroom = 0,# 房型\n",
    "             area = 0,# 面积\n",
    "             age = 0,# 楼龄\n",
    "             lift = 0# 电梯\n",
    "            )\n",
    "        response1 = get_url_contents(url_1)\n",
    "        soup = BeautifulSoup(response1.text, \"html.parser\")\n",
    "        amount = int(soup.find(\"h2\",{\"class\":\"total fl\"}).span.text)\n",
    "        pgnbr_to = int(amount)//30+1\n",
    "        print(\"%d pages in total\" % (pgnbr_to))\n",
    "             \n",
    "        for i in range(pgnbr_to): \n",
    "            url_to_scrape = LJ_urlmaker(urllist, nbr = 0,\n",
    "                                         pgnbr = i,\n",
    "                                         price = 0,# 售价\n",
    "                                         bedroom = 0,# 房型\n",
    "                                         area = 0,# 面积\n",
    "                                         age = 0,# 楼龄\n",
    "                                         lift = 0# 电梯\n",
    "                                       )\n",
    "            print(\"scraping:\"+url_to_scrape+\"...\")\n",
    "            response = get_url_contents(url_to_scrape)    \n",
    "            result_df = LJ_get_dataframe(response)\n",
    "            result_df.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        result_df1=pd.concat([result_df,result_df1],axis=0)\n",
    "        result_df1.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    print(\"get info finished!!\")\n",
    "    return result_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 pages in total\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "32 pages in total\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "get info finished!!\n"
     ]
    }
   ],
   "source": [
    "final = get_info(urllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final.profile_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝壳找房"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems like BK and LJ have same website structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BK_urllist= [\"https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/\",# Shuang jing\n",
    "          \"https://bj.ke.com/ershoufang/rs%E5%8A%B2%E6%9D%BE/\" # Jin song\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BK_get_dataframe(response):    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    comment_list = soup.find_all(\"div\",{\"class\":\"title\"})\n",
    "    location_list = soup.find_all(\"div\",{\"class\":\"positionInfo\"})\n",
    "#     address_list = soup.find_all(\"div\",{\"class\":\"address\"})\n",
    "    houseInfo_list = soup.find_all(\"div\",{\"class\":\"houseInfo\"})\n",
    "    followInfo_list = soup.find_all(\"div\",{\"class\":\"followInfo\"})\n",
    "    tag_list = soup.find_all(\"div\",{\"class\":\"tag\"})\n",
    "    priceInfo_list = soup.find_all(\"div\",{\"class\":\"priceInfo\"})\n",
    "    \n",
    "    for i in range(len(location_list)):\n",
    "            \n",
    "#         cut address and price information\n",
    "        address_info = houseInfo_list[0].text.replace(\"\\n\",\"\").replace(\" \",\"\").split(\"|\")\n",
    "        price_info = priceInfo_list[i].text.replace(\"\\n\",\"\").split(\"单价\")\n",
    "#         get url of this apt\n",
    "        url = comment_list[i].a['href']\n",
    "        \n",
    "        d = {\n",
    "             \"name\" : comment_list[i].text.replace(\"\\n\",\"\"), \n",
    "             \"area\" : address_info[3].split(\"平\")[0], \n",
    "             \"direction\": address_info[4],\n",
    "             \"location\":location_list[i].text.replace(\"\\n\",\"\"),\n",
    "             \"floorplan\": address_info[2], \n",
    "             \"high/low\": \"不适用\", \n",
    "             \"year\": address_info[1].split(\"年\")[0],\n",
    "             \"decoration\": \"不适用\",\n",
    "             \"type\": \"不适用\",\n",
    "            \"follow_info\":followInfo_list[i].text.replace(\"\\n\",\"\"),\n",
    "            \"tag\": tag_list[i].text.replace(\"\\n\",\"\"),\n",
    "             \"url\": url,\n",
    "             \"total_price\": price_info[0].split(\"万\")[0],\n",
    "             \"unit_price\": price_info[1].split(\"元\")[0],\n",
    "            \"source\":\"贝壳\"\n",
    "            }\n",
    "        df = df.append(d, ignore_index=True)\n",
    "#         print(\"get dataframe finished!\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# BK_get_dataframe(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BK_get_info(urllist): \n",
    "    \n",
    "    result_df1 = pd.DataFrame()\n",
    "    for j in range(len(urllist)):\n",
    "        url_1 =LJ_urlmaker(urllist, nbr = 0,\n",
    "             pgnbr = 1,\n",
    "             price = 0,# 售价\n",
    "             bedroom = 0,# 房型\n",
    "             area = 0,# 面积\n",
    "             age = 0,# 楼龄\n",
    "             lift = 0# 电梯\n",
    "            )\n",
    "        response1 = get_url_contents(url_1)\n",
    "        soup = BeautifulSoup(response1.text, \"html.parser\")\n",
    "        amount = int(soup.find(\"h2\",{\"class\":\"total fl\"}).span.text)\n",
    "        pgnbr_to = int(amount)//30+1\n",
    "        print(\"%d pages in total\" % (pgnbr_to))\n",
    "             \n",
    "        for i in range(pgnbr_to): \n",
    "            url_to_scrape = LJ_urlmaker(urllist, nbr = 0,\n",
    "                                         pgnbr = i,\n",
    "                                         price = 0,# 售价\n",
    "                                         bedroom = 0,# 房型\n",
    "                                         area = 0,# 面积\n",
    "                                         age = 0,# 楼龄\n",
    "                                         lift = 0# 电梯\n",
    "                                       )\n",
    "            print(\"scraping:\"+url_to_scrape+\"...\")\n",
    "            response = get_url_contents(url_to_scrape)    \n",
    "            result_df = BK_get_dataframe(response)\n",
    "            result_df.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        result_df1=pd.concat([result_df,result_df1],axis=0)\n",
    "        result_df1.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    print(\"get info finished!!\")\n",
    "    return result_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 pages in total\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "32 pages in total\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "scraping:https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/...\n",
      "get info finished!!\n"
     ]
    }
   ],
   "source": [
    "final1 = BK_get_info(BK_urllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 15)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
