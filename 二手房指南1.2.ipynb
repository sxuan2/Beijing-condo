{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a combination of:\n",
    "* https://bj.lianjia.com/\n",
    "* https://bj.ke.com/\n",
    "* https://bj.fang.com/\n",
    "\n",
    "\n",
    "Automation of collecting house for sale data and some analysis if needed\n",
    "\n",
    "Since in Beijing new construction is pretty far we are not cosidering it now.    \n",
    "Mainly focus on used house/apt around Shuangjing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import matplotlib\n",
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 链家找房"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urllist contains all urls that should be go over\n",
    "# It includes all different areas: Shuangjing, Jinsong, etc\n",
    "\n",
    "urllist= [\"https://bj.lianjia.com/ershoufang/rs%E5%8F%8C%E4%BA%95/\",# Shuang jing\n",
    "          \"https://bj.lianjia.com/ershoufang/rs%E5%8A%B2%E6%9D%BE/\" # Jin song\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LJ_urlmaker(urllist, nbr = 0,\n",
    "             pgnbr = 1,\n",
    "             price = 0,# 售价\n",
    "             bedroom = 0,# 房型\n",
    "             area = 0,# 面积\n",
    "             age = 0,# 楼龄\n",
    "             lift = 0# 电梯\n",
    "            ):\n",
    "    \n",
    "    url = urllist[nbr]\n",
    "    url1 = url.split(\"/\")[-2]    \n",
    "    \n",
    "    if price == 1:\n",
    "        price_suffix = \"p1\" # <200\n",
    "    elif price  == 2:\n",
    "        price_suffix = \"p2\" # 200-250\n",
    "    elif price  == 3:\n",
    "        price_suffix = \"p3\" # 250-300\n",
    "    elif price  == 4:\n",
    "        price_suffix = \"p4\" # 300-400\n",
    "    elif price  == 5:\n",
    "        price_suffix= \"p5\" # 400-500\n",
    "    elif price  == 6:\n",
    "        price_suffix= \"p6\" # 500-800\n",
    "    elif price  == 7:\n",
    "        price_suffix= \"p7\" # 800-1000\n",
    "    elif price  == 8:\n",
    "        price_suffix= \"p8\" # >1000\n",
    "    else:\n",
    "        price_suffix =\"\"\n",
    "        \n",
    "    if bedroom == 1:\n",
    "        bd_suffix =  \"l1\"\n",
    "    elif bedroom  == 2:\n",
    "        bd_suffix =  \"l2\"\n",
    "    elif bedroom  == 3:\n",
    "        bd_suffix =  \"l3\"\n",
    "    elif bedroom  == 4:\n",
    "        bd_suffix =  \"l4\"\n",
    "    elif bedroom  == 5:\n",
    "        bd_suffix =  \"l5\"\n",
    "    else:\n",
    "        bd_suffix =\"\"\n",
    "\n",
    "    if area == 1:\n",
    "        area_suffix = \"a1\" # <50\n",
    "    elif area  == 2:\n",
    "        area_suffix = \"a2\" # 50-70\n",
    "    elif area  == 3:\n",
    "        area_suffix = \"a3\" # 70-90\n",
    "    elif area  == 4:\n",
    "        area_suffix = \"a4\" # 90-110\n",
    "    elif area  == 5:\n",
    "        area_suffix= \"a5\" # 110-130\n",
    "    elif area  == 6:\n",
    "        area_suffix= \"a6\" # 130-150\n",
    "    elif area  == 7:\n",
    "        area_suffix= \"a7\" # 150-200\n",
    "    elif area  == 8:\n",
    "        area_suffix= \"a8\" # >200\n",
    "    else:\n",
    "        area_suffix =\"\"\n",
    "\n",
    "    if age == 1:\n",
    "        age_suffix =  \"y1\" # <5\n",
    "    elif bedroom  == 2:\n",
    "        age_suffix =  \"y2\" # <10\n",
    "    elif bedroom  == 3:\n",
    "        age_suffix =  \"y3\" # <15\n",
    "    elif bedroom  == 4:\n",
    "        age_suffix =  \"y4\" # <20\n",
    "    elif bedroom  == 5:\n",
    "        age_suffix =  \"y5\" # >20\n",
    "    else:\n",
    "        age_suffix =\"\"\n",
    "\n",
    "    if lift == 1:\n",
    "        lift_suffix =  \"ie2\" # has lift\n",
    "    elif lift  == 2:\n",
    "        lift_suffix =  \"ie1\" # no lift\n",
    "    else:\n",
    "        lift_suffix =\"\"\n",
    "    \n",
    "    \n",
    "    url2 = \"https://bj.lianjia.com/ershoufang/\" + \"pg\" + str(pgnbr)+ price_suffix + bd_suffix + area_suffix + age_suffix + lift_suffix + url1 + \"/\"\n",
    "    \n",
    "    return url2\n",
    "\n",
    "# a = LJ_urlmaker(urllist, nbr = 0,\n",
    "#              pgnbr = 3,\n",
    "#              price = 0,# 售价\n",
    "#              bedroom = 0,# 房型\n",
    "#              area = 0,# 面积\n",
    "#              age = 0,# 楼龄\n",
    "#              lift = 0# 电梯\n",
    "#             )\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_contents(url_to_scrape):\n",
    "    headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'\n",
    "            }\n",
    "    \n",
    "    response = requests.get(url_to_scrape,headers=headers,timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        return response\n",
    "    else:\n",
    "        print(\"internet problem!\")\n",
    "\n",
    "# aa = get_url_contents(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LJ_get_dataframe(response):    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    comment_list = soup.find_all(\"div\",{\"class\":\"title\"})\n",
    "    location_list = soup.find_all(\"div\",{\"class\":\"positionInfo\"})\n",
    "    address_list = soup.find_all(\"div\",{\"class\":\"address\"})\n",
    "#     houseInfo_list = soup.find_all(\"div\",{\"class\":\"houseInfo\"})\n",
    "    followInfo_list = soup.find_all(\"div\",{\"class\":\"followInfo\"})\n",
    "    tag_list = soup.find_all(\"div\",{\"class\":\"tag\"})\n",
    "    priceInfo_list = soup.find_all(\"div\",{\"class\":\"priceInfo\"})\n",
    "    \n",
    "    for i in range(len(location_list)):\n",
    "            \n",
    "#         cut address and price information\n",
    "        address_info = address_list[i].text.split(\"|\")\n",
    "        price_info = priceInfo_list[i].text.split(\"单价\")\n",
    "#         get url of this apt\n",
    "        url = comment_list[i].a['href']\n",
    "        \n",
    "        d = {\n",
    "             \"name\" : comment_list[i].text, \n",
    "             \"area\" : address_info[1].split(\"平\")[0], \n",
    "             \"direction\": address_info[2],\n",
    "             \"location\":location_list[i].text,\n",
    "             \"floorplan\": address_info[0], \n",
    "             \"high/low\": address_info[4], \n",
    "             \"year\": address_info[5].split(\"年\")[0],\n",
    "             \"decoration\": address_info[3],\n",
    "             \"type\": address_info[6],\n",
    "            \"follow_info\":followInfo_list[i].text,\n",
    "            \"tag\": tag_list[i].text,\n",
    "             \"url\": url,\n",
    "             \"total_price\": price_info[0].split(\"万\")[0],\n",
    "             \"unit_price\": price_info[1].split(\"元\")[0],\n",
    "            \"source\":\"链家\"\n",
    "            }\n",
    "        df = df.append(d, ignore_index=True)\n",
    "#         print(\"get dataframe finished!\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# LJ_get_dataframe(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(urllist): \n",
    "    \n",
    "    result_df1 = pd.DataFrame()\n",
    "    for j in range(len(urllist)):\n",
    "#         print(\"抓取urllist中的第{}个网址...\".format(j))\n",
    "        url_1 =LJ_urlmaker(urllist, nbr = j,\n",
    "             pgnbr = 1,\n",
    "             price = 0,# 售价\n",
    "             bedroom = 0,# 房型\n",
    "             area = 0,# 面积\n",
    "             age = 0,# 楼龄\n",
    "             lift = 0# 电梯\n",
    "            )\n",
    "        response1 = get_url_contents(url_1)\n",
    "        soup = BeautifulSoup(response1.text, \"html.parser\")\n",
    "        amount = int(soup.find(\"h2\",{\"class\":\"total fl\"}).span.text)\n",
    "        pgnbr_to = int(amount)//30+1\n",
    "        print(\"%d pages in total\" % (pgnbr_to))\n",
    "             \n",
    "        for i in range(pgnbr_to): \n",
    "            print(\"抓取urllist中的第{}个网址的第{}个房源 ...\".format(j,i))\n",
    "            try:\n",
    "                url_to_scrape = LJ_urlmaker(urllist, nbr = 0,\n",
    "                                             pgnbr = i,\n",
    "                                             price = 0,# 售价\n",
    "                                             bedroom = 0,# 房型\n",
    "                                             area = 0,# 面积\n",
    "                                             age = 0,# 楼龄\n",
    "                                             lift = 0# 电梯\n",
    "                                           )\n",
    "                print(\"scraping:\"+url_to_scrape+\"...\")\n",
    "                response = get_url_contents(url_to_scrape)    \n",
    "                result_df = LJ_get_dataframe(response)\n",
    "\n",
    "                result_df1 = result_df1.append(result_df, ignore_index=True)\n",
    "                \n",
    "#                 result_df1=pd.concat([result_df,result_df1],axis=0)\n",
    "#                 result_df1.reset_index(drop = True, inplace = True)\n",
    "#                 print(result_df1.head())\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "\n",
    "    \n",
    "    print(\"get info finished!!\")\n",
    "    return result_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 pages in total\n",
      "抓取urllist中的第0个网址的第0个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg0rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第1个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg1rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第2个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg2rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第3个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg3rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第4个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg4rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第5个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg5rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第6个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg6rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第7个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg7rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第8个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg8rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第9个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg9rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第10个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg10rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第11个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg11rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第12个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg12rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第13个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg13rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第14个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg14rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第15个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg15rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第16个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg16rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第17个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg17rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第18个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg18rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第19个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg19rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第20个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg20rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第21个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg21rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第22个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg22rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第23个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg23rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第24个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg24rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第25个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg25rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第26个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg26rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第27个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg27rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第28个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg28rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第29个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg29rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第30个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg30rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第31个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg31rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第32个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg32rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第33个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg33rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第0个网址的第34个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg34rs%E5%8F%8C%E4%BA%95/...\n",
      "24 pages in total\n",
      "抓取urllist中的第1个网址的第0个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg0rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第1个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg1rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第2个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg2rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第3个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg3rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第4个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg4rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第5个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg5rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第6个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg6rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第7个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg7rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第8个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg8rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第9个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg9rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第10个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg10rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第11个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg11rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第12个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg12rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第13个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg13rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第14个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg14rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第15个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg15rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第16个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg16rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第17个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg17rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第18个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg18rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第19个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg19rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第20个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg20rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第21个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg21rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第22个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg22rs%E5%8F%8C%E4%BA%95/...\n",
      "抓取urllist中的第1个网址的第23个房源 ...\n",
      "scraping:https://bj.lianjia.com/ershoufang/pg23rs%E5%8F%8C%E4%BA%95/...\n",
      "get info finished!!\n"
     ]
    }
   ],
   "source": [
    "final = get_info(urllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1620, 15)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"test.csv\", encoding = \"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝壳找房"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems like BK and LJ have same website structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BK_urllist= [\"https://bj.ke.com/ershoufang/rs%E5%8F%8C%E4%BA%95/\",# Shuang jing\n",
    "          \"https://bj.ke.com/ershoufang/rs%E5%8A%B2%E6%9D%BE/\" # Jin song\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BK_get_dataframe(response):    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    comment_list = soup.find_all(\"div\",{\"class\":\"title\"})\n",
    "    location_list = soup.find_all(\"div\",{\"class\":\"positionInfo\"})\n",
    "#     address_list = soup.find_all(\"div\",{\"class\":\"address\"})\n",
    "    houseInfo_list = soup.find_all(\"div\",{\"class\":\"houseInfo\"})\n",
    "    followInfo_list = soup.find_all(\"div\",{\"class\":\"followInfo\"})\n",
    "    tag_list = soup.find_all(\"div\",{\"class\":\"tag\"})\n",
    "    priceInfo_list = soup.find_all(\"div\",{\"class\":\"priceInfo\"})\n",
    "    \n",
    "    for i in range(len(location_list)):\n",
    "            \n",
    "#         cut address and price information\n",
    "        address_info = houseInfo_list[0].text.replace(\"\\n\",\"\").replace(\" \",\"\").split(\"|\")\n",
    "        price_info = priceInfo_list[i].text.replace(\"\\n\",\"\").split(\"单价\")\n",
    "#         get url of this apt\n",
    "        url = comment_list[i].a['href']\n",
    "        \n",
    "        d = {\n",
    "             \"name\" : comment_list[i].text.replace(\"\\n\",\"\"), \n",
    "             \"area\" : address_info[3].split(\"平\")[0], \n",
    "             \"direction\": address_info[4],\n",
    "             \"location\":location_list[i].text.replace(\"\\n\",\"\"),\n",
    "             \"floorplan\": address_info[2], \n",
    "             \"high/low\": \"不适用\", \n",
    "             \"year\": address_info[1].split(\"年\")[0],\n",
    "             \"decoration\": \"不适用\",\n",
    "             \"type\": \"不适用\",\n",
    "            \"follow_info\":followInfo_list[i].text.replace(\"\\n\",\"\"),\n",
    "            \"tag\": tag_list[i].text.replace(\"\\n\",\"\"),\n",
    "             \"url\": url,\n",
    "             \"total_price\": price_info[0].split(\"万\")[0],\n",
    "             \"unit_price\": price_info[1].split(\"元\")[0],\n",
    "            \"source\":\"贝壳\"\n",
    "            }\n",
    "        df = df.append(d, ignore_index=True)\n",
    "#         print(\"get dataframe finished!\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# BK_get_dataframe(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BK_get_info(urllist): \n",
    "    \n",
    "    result_df1 = pd.DataFrame()\n",
    "    for j in range(len(urllist)):\n",
    "        url_1 =LJ_urlmaker(urllist, nbr = 0,\n",
    "             pgnbr = 1,\n",
    "             price = 0,# 售价\n",
    "             bedroom = 0,# 房型\n",
    "             area = 0,# 面积\n",
    "             age = 0,# 楼龄\n",
    "             lift = 0# 电梯\n",
    "            )\n",
    "        response1 = get_url_contents(url_1)\n",
    "        soup = BeautifulSoup(response1.text, \"html.parser\")\n",
    "        amount = int(soup.find(\"h2\",{\"class\":\"total fl\"}).span.text)\n",
    "        pgnbr_to = int(amount)//30+1\n",
    "        print(\"%d pages in total\" % (pgnbr_to))\n",
    "             \n",
    "        for i in range(pgnbr_to): \n",
    "            url_to_scrape = LJ_urlmaker(urllist, nbr = 0,\n",
    "                                         pgnbr = i,\n",
    "                                         price = 0,# 售价\n",
    "                                         bedroom = 0,# 房型\n",
    "                                         area = 0,# 面积\n",
    "                                         age = 0,# 楼龄\n",
    "                                         lift = 0# 电梯\n",
    "                                       )\n",
    "            print(\"scraping:\"+url_to_scrape+\"...\")\n",
    "            response = get_url_contents(url_to_scrape)    \n",
    "            result_df = BK_get_dataframe(response)\n",
    "            result_df.reset_index(drop = True, inplace = True)\n",
    "        \n",
    "            result_df1=pd.concat([result_df,result_df1],axis=0)\n",
    "            result_df1.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    print(\"get info finished!!\")\n",
    "    return result_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1 = BK_get_info(BK_urllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1.to_csv(\"test.csv\",encoding = \"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
